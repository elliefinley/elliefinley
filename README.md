# Emotional Alignment in Oncology: Ethical AI Research

## Overview

This repository supports a research proposal exploring the ethical and emotional risks of using large language models (LLMs) in grief-intensive and cognitively vulnerable clinical environments—specifically oncology.

The project was inspired by a lived experience involving cognitive distortion during high-stress interaction with an emotionally responsive LLM. The goal is to build a framework for safe, emotionally intelligent AI design in healthcare—prioritizing user safety, psychological dignity, and ethical deployment.

## Contents

- `IRB_Concept_Note.docx` – Initial study design and narrative-to-research transition
- `Narrative_Reflection.docx` – Personal experience motivating this research
- `Literature_Review_and_Gap.docx` – Annotated sources and analysis of existing research gaps
- `Research_Proposal_OpenAI.docx` – Targeted pitch for research partnership

## Research Lead

**Molly Meister, MD**  
Radiation Oncology Resident, UCSF  
mollymeister.mm@gmail.com | San Francisco, CA  

This work is currently conducted without funding or institutional sponsorship to maintain early-stage ethical clarity.

## Notes

This project uses large language models (e.g., ChatGPT) in a reflective and collaborative capacity. All materials are authored and interpreted by the researcher, with AI assistance transparently disclosed.
